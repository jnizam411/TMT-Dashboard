{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMT Dashboard - Optimized for Apple M-Series\n",
    "\n",
    "This notebook implements an institutional-grade LSTM-based stock prediction system,\n",
    "**optimized for Apple Silicon (M1/M2/M3/M4)** with:\n",
    "\n",
    "- **Metal Performance Shaders (MPS)** GPU acceleration\n",
    "- **Rust-based compute kernels** for 10-50x faster feature engineering\n",
    "- **Adaptive memory management** for variable RAM configurations\n",
    "- **Zero-copy tensor operations** for efficiency\n",
    "\n",
    "## Hardware Requirements\n",
    "- Apple M-series Mac (M1, M2, M3, M4) - or any Mac with MPS support\n",
    "- Minimum 8GB RAM (16GB+ recommended)\n",
    "- PyTorch 2.0+ with MPS backend\n",
    "\n",
    "## Installation\n",
    "```bash\n",
    "# Install Python dependencies\n",
    "pip install -r tmt_optimized/requirements.txt\n",
    "\n",
    "# Build Rust acceleration (optional but recommended)\n",
    "cd tmt_optimized/rust_core && ./build.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Import optimized modules and configure hardware\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add tmt_optimized to path\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "# TMT Optimized imports\n",
    "try:\n",
    "    from tmt_optimized import (\n",
    "        MPSAccelerator,\n",
    "        get_device,\n",
    "        optimize_for_memory,\n",
    "        OptimizedFeatureEngine,\n",
    "        AdaptiveMemoryManager,\n",
    "        MemoryConfig,\n",
    "        OptimizedLSTM,\n",
    "        MPSTrainer,\n",
    "    )\n",
    "    from tmt_optimized.lstm_model import ModelConfig, TrainingConfig\n",
    "    TMT_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"TMT Optimized not available: {e}\")\n",
    "    print(\"Using fallback mode...\")\n",
    "    TMT_AVAILABLE = False\n",
    "\n",
    "# Configure matplotlib\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TMT DASHBOARD - APPLE SILICON OPTIMIZED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HARDWARE DETECTION AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARDWARE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize MPS Accelerator\n",
    "if TMT_AVAILABLE:\n",
    "    accelerator = MPSAccelerator(prefer_mps=True, memory_fraction=0.8)\n",
    "    device = accelerator.device\n",
    "    device_info = accelerator.device_info\n",
    "    \n",
    "    print(f\"\\nDevice: {device_info.device_name}\")\n",
    "    print(f\"Total Memory: {device_info.total_memory_gb:.1f} GB\")\n",
    "    print(f\"Available Memory: {device_info.available_memory_gb:.1f} GB\")\n",
    "    print(f\"Unified Memory: {device_info.unified_memory}\")\n",
    "    print(f\"Float16 Support: {device_info.supports_float16}\")\n",
    "else:\n",
    "    # Fallback device detection\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA: {torch.cuda.get_device_name()}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "# Initialize Memory Manager\n",
    "if TMT_AVAILABLE:\n",
    "    memory_config = MemoryConfig(\n",
    "        max_memory_gb=device_info.total_memory_gb if TMT_AVAILABLE else 8.0,\n",
    "        min_batch_size=8,\n",
    "        max_batch_size=256,\n",
    "        enable_checkpointing=True,\n",
    "    )\n",
    "    memory_manager = AdaptiveMemoryManager(memory_config)\n",
    "    \n",
    "    # Get recommended settings\n",
    "    settings = optimize_for_memory(device_info.available_memory_gb)\n",
    "    print(f\"\\nRecommended Settings for {device_info.available_memory_gb:.1f} GB:\")\n",
    "    for key, value in settings.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Target asset and date range\n",
    "TARGET_TICKER = 'AAPL'\n",
    "START_DATE = '2010-01-01'\n",
    "END_DATE = '2025-12-01'\n",
    "\n",
    "# Intermarket assets for analysis\n",
    "INTERMARKET_TICKERS = {\n",
    "    'target': TARGET_TICKER,\n",
    "    'rates': '^TNX',           # 10-Year Treasury Yield\n",
    "    'dollar': 'DX-Y.NYB',      # US Dollar Index\n",
    "    'vix': '^VIX',             # Volatility Index\n",
    "    'semis': 'SOXX',           # Semiconductor ETF\n",
    "    'tech': 'XLK',             # Technology Sector\n",
    "    'market': 'SPY'            # S&P 500 (benchmark)\n",
    "}\n",
    "\n",
    "print(f\"Target: {TARGET_TICKER}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Intermarket Assets: {len(INTERMARKET_TICKERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1: DATA DOWNLOAD WITH PROGRESS TRACKING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: DOWNLOADING MULTI-ASSET DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data_dict = {}\n",
    "download_times = {}\n",
    "\n",
    "for name, ticker in INTERMARKET_TICKERS.items():\n",
    "    print(f\"\\nDownloading {name.upper()}: {ticker}...\", end=\" \")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
    "        \n",
    "        # Handle MultiIndex columns\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "        \n",
    "        if not df.empty:\n",
    "            data_dict[name] = df\n",
    "            download_time = time.time() - start_time\n",
    "            download_times[name] = download_time\n",
    "            print(f\"✓ {len(df)} records ({download_time:.2f}s)\")\n",
    "        else:\n",
    "            print(\"✗ No data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal download time: {sum(download_times.values()):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1: DATA ALIGNMENT AND MERGING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALIGNING AND MERGING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find common dates\n",
    "common_dates = None\n",
    "for name, df in data_dict.items():\n",
    "    if common_dates is None:\n",
    "        common_dates = df.index\n",
    "    else:\n",
    "        common_dates = common_dates.intersection(df.index)\n",
    "\n",
    "print(f\"\\nCommon trading dates: {len(common_dates)}\")\n",
    "print(f\"Date range: {common_dates[0]} to {common_dates[-1]}\")\n",
    "\n",
    "# Align all dataframes\n",
    "aligned_data = {}\n",
    "for name, df in data_dict.items():\n",
    "    aligned_df = df.loc[common_dates].copy()\n",
    "    aligned_df.columns = [f\"{name}_{col}\" for col in aligned_df.columns]\n",
    "    aligned_data[name] = aligned_df\n",
    "    print(f\"  {name.upper()}: {aligned_df.shape}\")\n",
    "\n",
    "# Merge all\n",
    "merged_df = pd.concat(aligned_data.values(), axis=1)\n",
    "print(f\"\\nMerged shape: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 2: OPTIMIZED FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: OPTIMIZED FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize optimized feature engine\n",
    "if TMT_AVAILABLE:\n",
    "    from tmt_optimized.feature_engine import FeatureConfig\n",
    "    \n",
    "    feature_config = FeatureConfig(\n",
    "        ffd_d_values=[0.3, 0.4, 0.5],\n",
    "        rolling_windows=[5, 10, 20, 50],\n",
    "        use_rust=True,\n",
    "        parallel=True,\n",
    "    )\n",
    "    engine = OptimizedFeatureEngine(feature_config)\n",
    "else:\n",
    "    engine = None\n",
    "\n",
    "# Time the feature engineering\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract target data\n",
    "target_df = merged_df[[col for col in merged_df.columns if col.startswith('target_')]].copy()\n",
    "target_df.columns = [col.replace('target_', '') for col in target_df.columns]\n",
    "\n",
    "# Extract market data for neutralization\n",
    "market_df = merged_df[[col for col in merged_df.columns if col.startswith('market_')]].copy()\n",
    "market_df.columns = [col.replace('market_', '') for col in market_df.columns]\n",
    "\n",
    "features = pd.DataFrame(index=merged_df.index)\n",
    "\n",
    "# Calculate returns\n",
    "returns = target_df['Close'].pct_change().values\n",
    "log_prices = np.log(target_df['Close'].values)\n",
    "\n",
    "print(\"\\nEngineering features...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: FFD (Fractional Differentiation)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1. Fractional Differentiation (FFD)...\")\n",
    "ffd_start = time.time()\n",
    "\n",
    "if engine is not None:\n",
    "    # Use optimized Rust/Numba implementation\n",
    "    ffd_results = engine.frac_diff_ffd_batch(log_prices)\n",
    "    for name, values in ffd_results.items():\n",
    "        # Align lengths\n",
    "        padded = np.concatenate([np.full(len(merged_df) - len(values), np.nan), values])\n",
    "        features[name] = padded\n",
    "else:\n",
    "    # Fallback to pure NumPy\n",
    "    for d in [0.3, 0.4, 0.5]:\n",
    "        # Simple FFD implementation\n",
    "        weights = [1.0]\n",
    "        k = 1\n",
    "        while True:\n",
    "            w = -weights[-1] * (d - k + 1) / k\n",
    "            if abs(w) < 1e-5:\n",
    "                break\n",
    "            weights.append(w)\n",
    "            k += 1\n",
    "        weights = np.array(weights)\n",
    "        width = len(weights)\n",
    "        result = np.convolve(log_prices, weights[::-1], mode='valid')\n",
    "        padded = np.concatenate([np.full(len(merged_df) - len(result), np.nan), result])\n",
    "        features[f'ffd_{d}'] = padded\n",
    "\n",
    "ffd_time = time.time() - ffd_start\n",
    "print(f\"   FFD completed in {ffd_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: Microstructure & Technical Indicators\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2. Microstructure Features...\")\n",
    "micro_start = time.time()\n",
    "\n",
    "high = target_df['High'].values\n",
    "low = target_df['Low'].values\n",
    "close = target_df['Close'].values\n",
    "volume = target_df['Volume'].values\n",
    "\n",
    "if engine is not None:\n",
    "    features['order_flow_imbalance'] = engine.compute_order_flow_imbalance(high, low, close, volume)\n",
    "    features['quote_pressure'] = engine.compute_quote_pressure(high, low, close)\n",
    "else:\n",
    "    range_hl = high - low + 1e-10\n",
    "    position = (close - low) / range_hl\n",
    "    features['order_flow_imbalance'] = (2 * position - 1) * volume\n",
    "    features['quote_pressure'] = 2 * position - 1\n",
    "\n",
    "print(f\"   Microstructure completed in {time.time() - micro_start:.3f}s\")\n",
    "\n",
    "print(\"\\n3. Technical Indicators...\")\n",
    "tech_start = time.time()\n",
    "\n",
    "if engine is not None:\n",
    "    # RSI\n",
    "    features['rsi'] = engine.compute_rsi(close)\n",
    "    \n",
    "    # MACD\n",
    "    macd, signal, hist = engine.compute_macd(close)\n",
    "    features['macd'] = macd\n",
    "    features['macd_signal'] = signal\n",
    "    features['macd_hist'] = hist\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    for period in [20, 50]:\n",
    "        _, _, _, position = engine.compute_bollinger_bands(close, period)\n",
    "        features[f'bb_position_{period}'] = position\n",
    "    \n",
    "    # Rolling statistics\n",
    "    features['realized_vol'] = engine.rolling_std(returns, 20) * np.sqrt(252)\n",
    "else:\n",
    "    # Fallback implementations\n",
    "    # RSI\n",
    "    delta = pd.Series(close).diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    features['rsi'] = 100 - (100 / (1 + rs)).values\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = pd.Series(close).ewm(span=12).mean()\n",
    "    ema26 = pd.Series(close).ewm(span=26).mean()\n",
    "    features['macd'] = (ema12 - ema26).values\n",
    "    features['macd_signal'] = pd.Series(features['macd']).ewm(span=9).mean().values\n",
    "    features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    for period in [20, 50]:\n",
    "        ma = pd.Series(close).rolling(period).mean()\n",
    "        std = pd.Series(close).rolling(period).std()\n",
    "        features[f'bb_position_{period}'] = ((close - ma) / (std + 1e-10)).values\n",
    "    \n",
    "    features['realized_vol'] = pd.Series(returns).rolling(20).std().values * np.sqrt(252)\n",
    "\n",
    "print(f\"   Technical indicators completed in {time.time() - tech_start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: Momentum & Volume\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n4. Momentum Features...\")\n",
    "for period in [5, 10, 20]:\n",
    "    features[f'momentum_{period}'] = pd.Series(close).pct_change(period).values\n",
    "\n",
    "print(\"\\n5. Volume Features...\")\n",
    "if engine is not None:\n",
    "    vol_mean = engine.rolling_mean(volume.astype(np.float64), 20)\n",
    "else:\n",
    "    vol_mean = pd.Series(volume).rolling(20).mean().values\n",
    "features['volume_ratio'] = volume / (vol_mean + 1e-10)\n",
    "\n",
    "print(\"\\n6. Entropy Features...\")\n",
    "entropy_start = time.time()\n",
    "if engine is not None:\n",
    "    features['return_entropy'] = engine.rolling_entropy(returns, 20)\n",
    "else:\n",
    "    from scipy.stats import entropy\n",
    "    def rolling_entropy(x, window=20, bins=10):\n",
    "        result = np.full(len(x), np.nan)\n",
    "        for i in range(window-1, len(x)):\n",
    "            window_data = x[i-window+1:i+1]\n",
    "            window_data = window_data[~np.isnan(window_data)]\n",
    "            if len(window_data) >= 5:\n",
    "                hist, _ = np.histogram(window_data, bins=bins, density=True)\n",
    "                hist = hist[hist > 0]\n",
    "                result[i] = entropy(hist)\n",
    "        return result\n",
    "    features['return_entropy'] = rolling_entropy(returns)\n",
    "print(f\"   Entropy completed in {time.time() - entropy_start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: Intermarket Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n7. Intermarket Features...\")\n",
    "\n",
    "# Rates (Treasury Yield)\n",
    "if 'rates_Close' in merged_df.columns:\n",
    "    features['rates_diff'] = merged_df['rates_Close'].diff().values\n",
    "    features['rates_diff_ma5'] = pd.Series(features['rates_diff']).rolling(5).mean().values\n",
    "\n",
    "# VIX\n",
    "if 'vix_Close' in merged_df.columns:\n",
    "    features['vix_level'] = merged_df['vix_Close'].values\n",
    "    features['vix_pct_change'] = merged_df['vix_Close'].pct_change().values\n",
    "    vix_ma = merged_df['vix_Close'].rolling(252).mean()\n",
    "    vix_std = merged_df['vix_Close'].rolling(252).std()\n",
    "    features['vix_zscore'] = ((merged_df['vix_Close'] - vix_ma) / (vix_std + 1e-10)).values\n",
    "\n",
    "# Tech relative strength\n",
    "if 'tech_Close' in merged_df.columns and 'market_Close' in merged_df.columns:\n",
    "    tech_ret = merged_df['tech_Close'].pct_change()\n",
    "    spy_ret = merged_df['market_Close'].pct_change()\n",
    "    features['tech_rel_strength'] = (tech_ret - spy_ret).values\n",
    "\n",
    "# Semis momentum\n",
    "if 'semis_Close' in merged_df.columns:\n",
    "    features['semis_momentum_10'] = merged_df['semis_Close'].pct_change(10).values\n",
    "\n",
    "print(f\"   Intermarket features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: Target & Standardization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n8. Target Variable...\")\n",
    "features['target_1d'] = target_df['Close'].pct_change(1).shift(-1).values\n",
    "\n",
    "print(\"\\n9. Standardizing Features...\")\n",
    "std_start = time.time()\n",
    "for col in features.columns:\n",
    "    if col not in ['target_1d'] and not col.startswith('regime_'):\n",
    "        mean = features[col].expanding().mean()\n",
    "        std = features[col].expanding().std()\n",
    "        features[col] = (features[col] - mean) / (std + 1e-10)\n",
    "print(f\"   Standardization completed in {time.time() - std_start:.3f}s\")\n",
    "\n",
    "# Drop NaN rows\n",
    "features_clean = features.dropna()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total features: {len(features_clean.columns)}\")\n",
    "print(f\"Total samples: {len(features_clean)}\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Date range: {features_clean.index[0]} to {features_clean.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 3: DATA PREPARATION FOR LSTM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: PREPARING DATA FOR LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "SEQUENCE_LENGTH = settings.get('sequence_length', 30) if TMT_AVAILABLE else 30\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in features_clean.columns if not col.startswith('target_')]\n",
    "X = features_clean[feature_cols].values\n",
    "y = features_clean['target_1d'].values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Create sequences\n",
    "seq_start = time.time()\n",
    "if engine is not None:\n",
    "    X_seq, y_seq = engine.create_sequences(X, y, SEQUENCE_LENGTH)\n",
    "else:\n",
    "    # Fallback\n",
    "    n_sequences = len(X) - SEQUENCE_LENGTH\n",
    "    X_seq = np.zeros((n_sequences, SEQUENCE_LENGTH, X.shape[1]))\n",
    "    y_seq = np.zeros(n_sequences)\n",
    "    for i in range(n_sequences):\n",
    "        X_seq[i] = X[i:i+SEQUENCE_LENGTH]\n",
    "        y_seq[i] = y[i+SEQUENCE_LENGTH]\n",
    "\n",
    "print(f\"Sequence creation time: {time.time() - seq_start:.3f}s\")\n",
    "print(f\"X_seq shape: {X_seq.shape}\")\n",
    "print(f\"y_seq shape: {y_seq.shape}\")\n",
    "\n",
    "# Train/test split (temporal)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 4: MODEL TRAINING WITH MPS ACCELERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 4: TRAINING OPTIMIZED LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if TMT_AVAILABLE:\n",
    "    # Create optimized model\n",
    "    model_config = ModelConfig(\n",
    "        input_size=X_train.shape[2],\n",
    "        hidden_size=settings.get('hidden_size', 64),\n",
    "        num_layers=settings.get('num_layers', 2),\n",
    "        dropout=0.3,\n",
    "        use_layer_norm=True,\n",
    "    )\n",
    "    \n",
    "    training_config = TrainingConfig(\n",
    "        learning_rate=0.001,\n",
    "        batch_size=settings.get('batch_size', 32),\n",
    "        num_epochs=50,\n",
    "        gradient_clip=1.0,\n",
    "        early_stopping_patience=10,\n",
    "        gradient_accumulation_steps=settings.get('gradient_accumulation', 1),\n",
    "    )\n",
    "    \n",
    "    model = OptimizedLSTM(model_config)\n",
    "    trainer = MPSTrainer(\n",
    "        model=model,\n",
    "        training_config=training_config,\n",
    "        accelerator=accelerator,\n",
    "        memory_manager=memory_manager,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.train(\n",
    "        X_train=X_train.astype(np.float32),\n",
    "        y_train=y_train.astype(np.float32),\n",
    "        X_val=X_test.astype(np.float32),\n",
    "        y_val=y_test.astype(np.float32),\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    test_pred = trainer.predict(X_test.astype(np.float32))\n",
    "    \n",
    "else:\n",
    "    # Fallback to basic PyTorch training\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    class BasicLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                               batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            return self.fc(lstm_out[:, -1, :])\n",
    "    \n",
    "    model = BasicLSTM(X_train.shape[2]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).view(-1, 1).to(device)\n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, len(X_train_t), batch_size):\n",
    "            batch_X = X_train_t[i:i+batch_size]\n",
    "            batch_y = y_train_t[i:i+batch_size]\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss / n_batches)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/50], Loss: {epoch_loss/n_batches:.6f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_t).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 5: EVALUATION AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 5: EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate metrics\n",
    "if engine is not None:\n",
    "    test_ic = engine.compute_ic(test_pred, y_test)\n",
    "    strategy_returns = np.sign(test_pred) * y_test\n",
    "    test_sharpe = engine.compute_sharpe_ratio(strategy_returns)\n",
    "    test_max_dd = engine.compute_max_drawdown(strategy_returns)\n",
    "else:\n",
    "    test_ic = spearmanr(test_pred, y_test)[0]\n",
    "    strategy_returns = np.sign(test_pred) * y_test\n",
    "    test_sharpe = np.sqrt(252) * np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10)\n",
    "    cumulative = np.cumprod(1 + strategy_returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    test_max_dd = np.min((cumulative - running_max) / running_max)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Information Coefficient (IC): {test_ic:.4f}\")\n",
    "print(f\"  Sharpe Ratio: {test_sharpe:.4f}\")\n",
    "print(f\"  Max Drawdown: {test_max_dd*100:.2f}%\")\n",
    "print(f\"  Total Return: {np.sum(strategy_returns)*100:.2f}%\")\n",
    "print(f\"  Win Rate: {(strategy_returns > 0).mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Training Loss\n",
    "if TMT_AVAILABLE and 'train_loss' in history:\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].legend()\n",
    "else:\n",
    "    axes[0, 0].plot(history['train_loss'], linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions vs Actuals\n",
    "axes[0, 1].scatter(test_pred, y_test, alpha=0.5, s=10)\n",
    "axes[0, 1].plot([test_pred.min(), test_pred.max()],\n",
    "                [test_pred.min(), test_pred.max()],\n",
    "                'r--', linewidth=2, label='Perfect')\n",
    "axes[0, 1].set_title(f'Predictions vs Actuals (IC={test_ic:.3f})', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('Actual')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cumulative Returns\n",
    "cum_strategy = np.cumprod(1 + strategy_returns)\n",
    "cum_buyhold = np.cumprod(1 + y_test)\n",
    "axes[1, 0].plot(cum_strategy, label='Strategy', linewidth=2)\n",
    "axes[1, 0].plot(cum_buyhold, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].set_title('Cumulative Returns', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Days')\n",
    "axes[1, 0].set_ylabel('Cumulative Return')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Drawdown\n",
    "running_max = np.maximum.accumulate(cum_strategy)\n",
    "drawdown = (cum_strategy - running_max) / running_max\n",
    "axes[1, 1].fill_between(range(len(drawdown)), drawdown, 0, alpha=0.7, color='red')\n",
    "axes[1, 1].set_title(f'Drawdown (Max: {test_max_dd*100:.2f}%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Days')\n",
    "axes[1, 1].set_ylabel('Drawdown')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if TMT_AVAILABLE:\n",
    "    print(f\"\\nHardware:\")\n",
    "    print(f\"  Device: {device_info.device_name}\")\n",
    "    print(f\"  Memory Used: {memory_manager.get_memory_stats().used_gb:.2f} GB\")\n",
    "    print(f\"  Peak Memory: {memory_manager.get_memory_stats().peak_gb:.2f} GB\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Information Coefficient: {test_ic:.4f}\")\n",
    "print(f\"  Sharpe Ratio: {test_sharpe:.4f}\")\n",
    "print(f\"  Max Drawdown: {test_max_dd*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOptimization Status:\")\n",
    "print(f\"  Rust Core: {'Enabled' if (TMT_AVAILABLE and engine and engine.use_rust) else 'Disabled'}\")\n",
    "print(f\"  MPS Acceleration: {'Enabled' if str(device) == 'mps' else 'Disabled'}\")\n",
    "print(f\"  Adaptive Memory: {'Enabled' if TMT_AVAILABLE else 'Disabled'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
